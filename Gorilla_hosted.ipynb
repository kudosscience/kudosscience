{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kudosscience/kudosscience/blob/main/Gorilla_hosted.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gorilla Hosted - Try it out in less than 60s üöÄ\n",
        "\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ShishirPatil/gorilla)  [![arXiv](https://img.shields.io/badge/arXiv-2305.15334-<COLOR>.svg?style=flat-square)](https://arxiv.org/abs/2305.15334)   [![Discord](https://img.shields.io/discord/1111172801899012102?label=Discord&logo=discord&logoColor=green&style=flat-square)](https://discord.gg/SwTyuTAxX3)  [![Twitter](https://img.shields.io/twitter/url?url=https://twitter.com/shishirpatil_/status/1661780076277678082)](https://twitter.com/shishirpatil_/status/1661780076277678082)\n",
        "\n",
        "Play around with Gorilla! Here, we host the Gorilla zero-shot models, so you can try it out! This is compatible with the OpenAI chat completion API - plug and play!\n",
        "\n",
        "üü¢ Now with Apache-2.0! Gorilla is commercially usable with no obligations üöÄ\n",
        "\n",
        "We are happy to launch all three models: `gorilla-7b-hf-v1` which chooses from 925 Hugging Face APIs 0-shot, `gorilla-7b-th-v0` for 94 (exhaustive) Tensor Hub APIs 0-shot, `gorilla-7b-tf-v0` for 626 (exhaustive) Tensorflow Hub APIs 0-shot. `gorilla-mpt-7b-hf-v0` and `gorilla-falcon-7b-hf-v0`are two Apache-2.0 licensed models for Hugging Face APIs. We have a hosted end-point for `gorilla-mpt-7b-hf-v0` in this colab, and are in the process of adding `gorilla-falcon-7b-hf-v0` soon! In spirit of openess, we do not filter, nor carry out any post processing either to the prompt nor response. We will release the combined {HF+TF+TH} model which also has generic chat capability slowly to accomodate server demand.\n",
        "\n",
        "üíÉ If you want to use Gorilla or build on top of it! Feel absolutely free to do so - we believe in open source research and you don't even have to tell us! In case you choose to do, we have a vibrant community in Discord! Stop by and say Hi üëã\n",
        "\n",
        "<img src=\"https://github.com/ShishirPatil/gorilla/blob/gh-pages/assets/img/logo.png?raw=true\" width=30% height=30%>"
      ],
      "metadata": {
        "id": "7bKku43frr8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gorilla ü¶ç is hosted by UC Berkeley Sky lab for FREE ü§© as a research prototype ü§ì\n",
        "## Please don't use it for commercial serving üëÄ\n",
        "## The hosted models are only trained to serve HuggingFace/TF/Torch APIs. They are NOT trained to serve other restful APIs."
      ],
      "metadata": {
        "id": "5PA9GQbV4rcN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eBd_fso7qFPX"
      },
      "outputs": [],
      "source": [
        "# Import Chat completion template and set-up variables\n",
        "!pip install openai==0.28.1 &> /dev/null\n",
        "import openai\n",
        "import urllib.parse\n",
        "\n",
        "openai.api_key = \"EMPTY\" # Key is ignored and does not matter\n",
        "openai.api_base = \"http://zanino.millennium.berkeley.edu:8000/v1\"\n",
        "# Alternate mirrors\n",
        "# openai.api_base = \"http://34.132.127.197:8000/v1\"\n",
        "\n",
        "# Report issues\n",
        "def raise_issue(e, model, prompt):\n",
        "    issue_title = urllib.parse.quote(\"[bug] Hosted Gorilla: <Issue>\")\n",
        "    issue_body = urllib.parse.quote(f\"Exception: {e}\\nFailed model: {model}, for prompt: {prompt}\")\n",
        "    issue_url = f\"https://github.com/ShishirPatil/gorilla/issues/new?assignees=&labels=hosted-gorilla&projects=&template=hosted-gorilla-.md&title={issue_title}&body={issue_body}\"\n",
        "    print(f\"An exception has occurred: {e} \\nPlease raise an issue here: {issue_url}\")\n",
        "\n",
        "# Query Gorilla server\n",
        "def get_gorilla_response(prompt=\"I would like to translate from English to French.\", model=\"gorilla-7b-hf-v1\"):\n",
        "  try:\n",
        "    completion = openai.ChatCompletion.create(\n",
        "      model=model,\n",
        "      messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return completion.choices[0].message.content\n",
        "  except Exception as e:\n",
        "    raise_issue(e, model, prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßë‚Äçüíª [Update Jun 15] With our new v1 model `gorilla-7b-hf-delta-v1`, Gorilla now returns code snippets you can use directly in your workflow!"
      ],
      "metadata": {
        "id": "FNDBCAMBV0aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1: Translation ‚úç with ü§ó"
      ],
      "metadata": {
        "id": "asdPNq38qIx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gorilla `gorilla-mpt-7b-hf-v1` with code snippets\n",
        "# Translation\n",
        "prompt = \"I would like to translate 'I feel very good today.' from English to Chinese.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-hf-v1\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2lVWV9yWO0i",
        "outputId": "de08140b-8820-4ae5-d295-4df4de11dcb2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<<domain>>>: Natural Language Processing Translation\n",
            "<<<api_call>>>: translation = pipeline('translation_en_to_zh', model='Helsinki-NLP/opus-mt-en-zh')\n",
            "<<<api_provider>>>: Hugging Face Transformers\n",
            "<<<explanation>>>: 1. Import the necessary components from the Hugging Face Transformers library.\n",
            "2. Initialize the translation pipeline with the 'Helsinki-NLP/opus-mt-en-zh' model, which is designed for English to Chinese translation.\n",
            "3. Translate the given text from English to Chinese by passing the text to the translation pipeline.<<<code>>>:\n",
            "\n",
            "from transformers import pipeline\n",
            "\n",
            "def load_model():\n",
            "    model = pipeline('translation_en_to_zh', model='Helsinki-NLP/opus-mt-en-zh')\n",
            "    return model\n",
            "\n",
            "def process_data(text, model):\n",
            "    response = model(text, tgt_lang='zh')[0]['translation_text']\n",
            "    return response\n",
            "\n",
            "text = 'I feel very good today.'\n",
            "\n",
            "# Load the model\n",
            "model = load_model()\n",
            "\n",
            "# Process the data\n",
            "response = process_data(text, model)\n",
            "print(response)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2: Object detection üî∑ with ü§ó"
      ],
      "metadata": {
        "id": "Gnx7YHf18DTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gorilla `gorilla-7b-hf-v1` with code snippets\n",
        "# Object Detection\n",
        "prompt = \"I want to build a robot that can detecting objects in an image ‚Äòcat.jpeg‚Äô. Input: [‚Äòcat.jpeg‚Äô]\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-hf-v1\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvQawbSxWoEu",
        "outputId": "35765596-d9c6-4741-93be-27f5f6a913ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<<domain>>>: Computer Vision Object Detection\n",
            "<<<api_call>>>: model = OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\n",
            "<<<api_provider>>>: Hugging Face Transformers\n",
            "<<<explanation>>>: 1. Import the necessary libraries, including PyTorch, Transformers, PIL, and OwlViTProcessor and OwlViTForObjectDetection.\n",
            "2. Load the pretrained model 'google/owlvit-large-patch14' using OwlViTForObjectDetection.from_pretrained().\n",
            "3. Load the image 'cat.jpeg' and initialize the processor with OwlViTProcessor.from_pretrained().\n",
            "4. Process the input image and generate inputs for the model.\n",
            "5. Pass the inputs to the model and receive the predicted logits and bounding boxes.\n",
            "6. Display the detected objects in the image along with their bounding boxes.\n",
            "<<<code>>>:\n",
            "\n",
            "import torch\n",
            "from PIL import Image\n",
            "from transformers import ViTFeatureExtractor, ViTForObjectDetection\n",
            "\n",
            "def load_model():\n",
            "    feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-large-patch16-224')\n",
            "    model = ViTForObjectDetection.from_pretrained('google/vit-large-patch16-224')\n",
            "    return feature_extractor, model\n",
            "\n",
            "def process_data(image_path, feature_extractor, model):\n",
            "    image = Image.open(image_path)\n",
            "    inputs = feature_extractor(images=image, return_tensors='pt')\n",
            "    outputs = model(**inputs)\n",
            "    target_sizes = torch.tensor([image.size[::-1]])\n",
            "    results = feature_extractor.post_process(outputs=outputs, target_sizes=target_sizes)\n",
            "    response = [{'label': label, 'score': score, 'bbox': bbox} for label, score, bbox in zip(results[0]['labels'], results[0]['scores'], results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's try to invoke APIs from Torch Hub instead for the same prompts!"
      ],
      "metadata": {
        "id": "Ot4EKOLXhpoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Translation ‚úç with Torch Hub\n",
        "prompt = \"I would like to translate from English to Chinese.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-th-v0\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS-ZeP0rhmzp",
        "outputId": "d3b9c659-e2b0-4855-8ef9-1d247a2aa58c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'domain': 'Machine Translation', 'api_call': \\\"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_snnmlp', language='en', pretrained=True)\\\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained NVIDIA SNNLM-P model for English-to-Chinese machine translation from PyTorch Hub, which is trained on a large bilingual corpus and provides good translation accuracy.', 'code': 'import torch\\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_snnmlp', language='en', pretrained=True)'}\\n\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚õ≥Ô∏è With Gorilla being fine-tuned on MPT, and Falcon, you can use Gorilla commercially with no obligations! üü¢"
      ],
      "metadata": {
        "id": "hvQ3q5AX1wqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gorilla with `gorilla-mpt-7b-hf-v0`\n",
        "prompt = \"I would like to translate from English to Chinese.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-mpt-7b-hf-v0\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGi3wwmQ1voP",
        "outputId": "3bae3995-1446-49f3-c11c-7f0781d8fa43"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide the text you'd like to translate: 'Hello, world!'<|im_end|><|im_start|>assistant\n",
            "\\n<<<domain>>>: Natural Language Processing Text2Text Generation\\n<<<api_call>>>: M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_1.2B')\\n<<<api_provider>>>: Hugging Face Transformers\\n<<<explanation>>>: 1. Import the necessary libraries, which include M2M100ForConditionalGeneration and M2M100Tokenizer from transformers.\\n2. Use the from_pretrained method to load the pre-trained model 'facebook/m2m100_1.2B'.\\n3. Set the source language to English ('en') and use the tokenizer to tokenize the input text.\\n4. Generate the translated text using the model's generate method, and then use the tokenizer's batch_decode method to decode the output tokens into a readable string.\\n<<<code>>>: from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\\nsrc_text = \\\"Hello, world!\\\"\\nmodel = M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_1.2B')\\ntokenizer = M2M100Tokenizer.from_pretrained('facebook/m2m100_1.2B')\\ntokenizer.src_lang = \\\"en\\\"\\ninputs = tokenizer(src_text, return_tensors=\\\"pt\\\")\\noutputs = model.generate(**inputs)\\ntranslated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\\n\" \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We will deprecate the `gorilla-7b-hf-v0` model on July 4 when we will automatically upgrade all v0 model requests to v1. The only changes between v0 and v1 is better code snippets.\n",
        "Below are example prompt-responses for `gorilla-7b-hf-v0` Legacy Model for ü§ó"
      ],
      "metadata": {
        "id": "fIRsh6Ne9f0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I would like to translate from English to Chinese.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-hf-v0\" ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMnJzPaN5FlV",
        "outputId": "5d0adde4-b666-4923-d850-aea8a773bf2c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<<domain>>>: Natural Language Processing Text2Text Generation\n",
            "<<<api_call>>>: M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_2.7B')\n",
            "<<<api_provider>>>: Hugging Face Transformers\n",
            "<<<explanation>>>: 1. Import the necessary classes from the transformers package provided by Hugging Face. This includes M2M100ForConditionalGeneration for the text-to-text generation model and M2M100Tokenizer for tokenizing the input text.\n",
            "2. Load the pre-trained model 'facebook/m2m100_2.7B' using the from_pretrained method of the M2M100ForConditionalGeneration class. This model has been trained for multilingual translation tasks, which is what we need for translating from English to Chinese.\n",
            "3. Encode the input text with the M2M100Tokenizer, specifying the source language (English) and target language (Chinese) using the get_lang_id and padding methods.\n",
            "4. Use the model to generate translations by passing in the encoded input text. The result will be a translation in Chinese.\n",
            "<<<code>>>: from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
            "model = M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_2.7B')\n",
            "tokenizer = M2M100Tokenizer.from_pretrained('facebook/m2m100_2.7B')\n",
            "tokenizer.src_lang = 'en_XX'  # Replace with your source language code\n",
            "encoded_input = tokenizer(your_english_text, return_tensors='pt')\n",
            "outputs = model.generate(**encoded_input, forced_bos_token_id=tokenizer.get_lang_id('zh_XX'))\n",
            "translated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I want to build a robot that can detect objects in an image.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-hf-v0\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WochUPqf8HLa",
        "outputId": "bef6950d-c6ad-4946-9ec7-131712a1eeb9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<<domain>>>: Computer Vision Object Detection\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Using gorilla is as easy as calling `get_gorilla_response()` with your prompt! Try out Gorilla, and share your interesting findings in `#showcase` ü§© [Discord](https://discord.gg/3apqwwME)!"
      ],
      "metadata": {
        "id": "XS5Qe6zD8tdX"
      }
    }
  ]
}